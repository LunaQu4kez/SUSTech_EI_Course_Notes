# CS5025 Assignment 4

12541018 赵钊



## Problem 1

### 1.

这个现象主要由输入的分布特性导致。Christofides 算法虽然有 3/2 的最坏情况近似比保证，但它是一种通用且保守的算法，并没有针对特定数据集进行优化，所以对于现实世界常见的、结构较好的实例，解通常离最优解差的比较多。L2O 算法基于大量数据学习优化策略，能够捕捉到数据中的隐藏结构或常见模式，从而在这些常见实例上表现得更好。

### 2.

算法 B 的主要风险在于它没有最坏情况保证。

例子：假设算法 B 是在大量均匀分布在单位正方形内的欧几里得 TSP 实例上训练的，但部署时遇到特殊结构的实例：节点几乎分布在一个狭窄的环状或近似圆环上，并且节点间距差异极大。对于这种 “对抗性” 结构，算法 A 会比算法 B 表现更好。



## Problem 2

### 1.

**1)** 不一样。L2O 是学习一种启发式方法，使其能泛化到来自同一分布的问题，而不是仅仅适应单个实例。将训练集和测试集都设为同一个实例，会使 L2O 失去了泛化能力。

**2)** LKH-3 或 Concorde 是专门求解 TSP 的算法，在特定实例上可以快速得到高质量解。而训练模型需要大量迭代，涉及计算开销、调参和训练时间等成本。因此，对于一个固定的问题实例，训练模型的总时间大于运行经典优化算法，而且模型解的质量可能还不如经典方法。

### 2.

Scenario A：监督学习 + 梯度下降
因为有精确的最优路径作为标签（Ground Truth），且模型的预测输出（分类概率）与损失函数（分类误差）是可微分的，可以直接通过反向传播进行梯度优化。

Scenario B：强化学习
因为没有真实标签，只能通过采样生成的路径长度作为奖励信号，而模型的输出是离散的路径抽样，这个过程不可微分，强化学习适合优化这种不可微的回报目标。

Scenario C：梯度无关优化
遗传算法的内部过程是离散且不可微的，损失函数依赖于整个算法的运行结果，无法通过梯度反向传播，因此需要使用梯度无关的方法。

